{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muddasserniaz/miscfragments/blob/main/LLaMa3.1_Gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio transformers torch -q\n",
        "import os\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def get_huggingface_token():\n",
        "    \"\"\"\n",
        "    Fetch Hugging Face token from environment variables or prompt the user.\n",
        "    Returns:\n",
        "        str: Hugging Face token\n",
        "    Raises:\n",
        "        ValueError: If no valid token is provided.\n",
        "    \"\"\"\n",
        "    hf_token = os.getenv('HF_TOKEN')\n",
        "    if not hf_token:\n",
        "        logger.warning(\"HF_TOKEN environment variable is not set.\")\n",
        "        hf_token = input(\"Please enter your Hugging Face token: \").strip()\n",
        "        if not hf_token:\n",
        "            raise ValueError(\"A valid Hugging Face token is required to proceed.\")\n",
        "    return hf_token\n",
        "\n",
        "def load_model_and_tokenizer(model_name, token):\n",
        "    \"\"\"\n",
        "    Load the model and tokenizer using Hugging Face's Auto classes.\n",
        "    Args:\n",
        "        model_name (str): Hugging Face model name.\n",
        "        token (str): Hugging Face token.\n",
        "    Returns:\n",
        "        tuple: model, tokenizer, device\n",
        "    \"\"\"\n",
        "    try:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        logger.info(f\"Using device: {device}\")\n",
        "\n",
        "        # Load model and tokenizer using Auto classes\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token).to(device)\n",
        "\n",
        "        # Set or add padding token\n",
        "        if tokenizer.pad_token is None:\n",
        "            logger.info(\"Adding pad_token to the tokenizer.\")\n",
        "            tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
        "            model.resize_token_embeddings(len(tokenizer))  # Adjust model embeddings for new token\n",
        "\n",
        "        return model, tokenizer, device\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading model or tokenizer: {e}\")\n",
        "        raise\n",
        "\n",
        "def chatbot_fn(prompt, chatbot_history=[]):\n",
        "    \"\"\"\n",
        "    Chatbot function to handle user prompts and generate responses.\n",
        "    Args:\n",
        "        prompt (str): User input prompt.\n",
        "        chatbot_history (list): History of the conversation.\n",
        "    Returns:\n",
        "        tuple: Assistant's response, updated conversation history.\n",
        "    \"\"\"\n",
        "    if not prompt.strip():\n",
        "        return \"Please enter a valid prompt.\", chatbot_history\n",
        "\n",
        "    try:\n",
        "        if chatbot_history:\n",
        "            conversation = [item['content'] for item in chatbot_history]\n",
        "            input_text = \"\\n\".join(conversation) + f\"\\nUser: {prompt}\\nAssistant:\"\n",
        "        else:\n",
        "            input_text = f\"User: {prompt}\\nAssistant:\"\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            input_text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=1024,\n",
        "            padding=True\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs, max_new_tokens=150, pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "        response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        assistant_response = response_text.split(\"Assistant:\")[-1].strip()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating response: {e}\")\n",
        "        return f\"An error occurred: {e}\", chatbot_history\n",
        "\n",
        "    chatbot_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "    chatbot_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "    return assistant_response, chatbot_history\n",
        "\n",
        "# Initialize Hugging Face model and tokenizer\n",
        "try:\n",
        "    hf_token = get_huggingface_token()\n",
        "    #model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  # Model name\n",
        "    model_name = \"meta-llama/Llama-3.2-1B\"  # Model name\n",
        "    model, tokenizer, device = load_model_and_tokenizer(model_name, hf_token)\n",
        "except Exception as e:\n",
        "    logger.critical(\"Failed to initialize the model. Exiting.\")\n",
        "    raise\n",
        "\n",
        "# Define Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot_fn,\n",
        "    inputs=[\"text\", \"state\"],\n",
        "    outputs=[\"text\", \"state\"],\n",
        "    title=\"LLaMA 3.1-8B Instruct Chatbot (GPU-accelerated)\",\n",
        "    description=\"Chat with a LLaMA 3.1-based model using GPU acceleration!\",\n",
        "    allow_flagging=\"never\",\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "F4CXapYZJOMV",
        "outputId": "585816af-c13c-42c4-fd65-afca96c071bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:HF_TOKEN environment variable is not set.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cc577b6a8d7e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Initialize Hugging Face model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mhf_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_huggingface_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;31m#model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  # Model name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"meta-llama/Llama-3.2-1B\"\u001b[0m  \u001b[0;31m# Model name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-cc577b6a8d7e>\u001b[0m in \u001b[0;36mget_huggingface_token\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhf_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HF_TOKEN environment variable is not set.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mhf_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please enter your Hugging Face token: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhf_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A valid Hugging Face token is required to proceed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}